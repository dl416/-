# 神经网络参数及反向传播算法（一）

​		神经网络依靠前向传播算法获得对样本的预测，依靠反向传播算法更新自身参数。反向传播算法也称误差逆传播算法（error BackPropagation，简称BP）。

​		给定训练集$D=\{(x_1,y_1),(x_2,y_2),\dots,(x_n,y_n)\},x_i\in\R^d,y_i\in\R^l$ 即输入实例由 d 个属性描述，输出 l 维实值向量。如无所示给出了一个拥有 d 个输入单元、l 个输出神经元、q 个隐层神经元的多元前馈神经网络，其中输出层第 j 个神经元的阈值（bias）用 $\theta_j$ 表示，隐层第 h 个神经元的阈值用 $\gamma_h$ 表示。输入层第 i 个神经元与隐层第 h 个神经元之间的链接权值为 $\nu_{ih}$ ，隐层第 h 个神经元与输出层第 j 个神经元之间的链接权值为 $\omega_{hj}$ 。记隐层的第 h 个神经元接收到的输入为 $\alpha_h=\sum_{i=1}^{d}\nu_{ih}x_i$ ，输出层第 j 个神经元接收到的输入为 $\beta_j=\sum_{h=1}^{q}\omega_{hj}b_h$ ，其中 $b_h$ 为隐层第 h 非神经元的输出 $b_h=f(\alpha_h-\gamma_h)$ 。假设隐层和输出层神经元都用 Sigmoid 函数作为激活函数。

![](images/屏幕截图(17)_看图王.png)

​		对训练例子 $(x_k,y_k)$ ，假定神经网络的输出为 $\hat{y}_k=(\hat{y}_1^k，\hat{y}_2^k),\dots,\hat{y}_l^k$ ，即
$$
\hat{y}_j^k = f(\beta_j-\theta_j)
$$
f 表示激活函数，则网络在 $(x_k,y_k)$ 上的均方误差为
$$
E_k=\frac{1}{2}\sum_{j=1}^{l}(\hat{y}_j^k-y_j^k)^2
$$
​		图示中的网络总共包含了（d+l+1）q+l 个参数需要确定：输入层到隐层的 dxq 个权值、隐层到输出层的 qxl 个权值、q 个隐层神经元阈值、l 个输出层神经元阈值。BP 算法是一个迭代学习算法，在每一轮的采用广义的感知机学习规则对参数进行估计，即和（3）式类似，任意的参数 $\nu$ 的更新估计式为：
$$
\nu\leftarrow\nu+\Delta\nu
$$




* ## 隐层到输出层的连接权值 $\omega_{hj}$

​		BP 算法基于梯度下降策略（gradient descent），以目标的扶梯度方向对参数进行调整。对（2）式的误差 $E_k$ 给定学习率 $\eta$ ，有
$$
\Delta\omega_{hj}=-\eta\frac{\partial E_k}{\partial \omega_{hj}}
$$
注意到， $\omega_{hj}$ 先影响到第 j 的输出层神经元的输入值 $\beta_j$ ，再影响到其输出值 $\hat{y}_j^k$ ，然后才影响到 $E_k$ ，有
$$
\frac{\partial E_k}{\partial\omega_{hj}}=\frac{\partial E_k}{\partial \hat{y}_j^k}\sdot\frac{\partial \hat{y}_j^k}{\partial \beta_j}\sdot\frac{\partial \beta_j}{\partial \omega_{hj}}
$$
​		根据 $\beta_j$ 的定义，显然有
$$
\frac{\partial \beta_j}{\partial\omega_{hj}}=b_h
$$
​		由 Sigmoid 函数：
$$
f(x)=\frac{1}{1+e^{-x}}\\
f'(x) = \frac{0-(-e^{-x})}{(1+e^{-x})^2}=\frac{e^{-x}}{(1+e^{-x})^2}=f(x)(1-f(x))
$$
​	

于是根据 （1） （2） 式，有
$$
g_j =-\frac{\partial E_k}{\partial \hat{y}_j^k}\sdot\frac{\partial \hat{y}_j^k}{\partial \beta_j}\\
=-(\hat{y}_j^k-y_j^k)f'(\beta_j-\theta_j)
$$
由于 $\hat{y}_j^k = f(\beta_j-\theta_j)$ ，并上（7）式，
$$
g_j=-(\hat{y}_j^k-y_j^k)f(\beta_j-\theta_j)(1-f(\beta_j-\theta_j))
\\
=-(\hat{y}_j^k-y_j^k)\hat{y}_j^k(1-\hat{y}_j^k)\\
=\hat{y}_j^k(1-\hat{y}_j^k)(y_j^k-\hat{y}_j^k)
$$
​		将式（9）和式（6）代入（5），有
$$
\Delta\omega_{hj}=\eta g_jb_h
$$




* ## 输入层到隐层的连接权值 $\nu_{ih}$

​		由前向传播的计算式可以得出，对于 $\nu_{ih}$ 而言，有：
$$
\Delta\nu_{ih}=-\eta\frac{\partial E_k}{\partial \nu_{ih}}
$$
​		根据求导链式法则，从图中可以看出由于 $\nu_{ih}$ 连接的隐层神经元产生的输出 $b_h$ 给到了所有的输出层节点，每一个 $\hat{y}_j^k$ 都是 $\nu_{ih}$ 的函数，需要对每一个 $\hat{y}_j^k$ 求导：
$$
\frac{\partial E_k}{\partial\nu_{ih}}=\sum_{j=1}^{l}\frac{\partial E_k}{\partial \hat{y}_j^k}\sdot\frac{\partial \hat{y}_j^k}{\partial \beta_j}\sdot\frac{\partial \beta_j}{\partial b_h}\sdot\frac{\partial b_h}{\partial \alpha_h}\sdot\frac{\partial \alpha_h}{\partial \nu_{ih}}
$$
​		由（8）（9）可知，
$$
\sum_{j=1}^{l}\frac{\partial E_k}{\partial \hat{y}_j^k}\sdot\frac{\partial \hat{y}_j^k}{\partial \beta_j}=\sum_{j=1}^{l}g_j
$$
​		因为 $\beta_j=\sum_{h=1}^{q}\omega_{hj}b_h$ ，
$$
\frac{\partial \beta_j}{\partial b_h}=\omega_{hj}
$$
​		又 $b_h=f(\alpha_h-\gamma_h)$ ， $\alpha_h=\sum_{i=1}^{d}\nu_{ih}x_i$ ，$f'(x) =f(x)(1-f(x))$ ，
$$
\frac{\partial b_h}{\partial \alpha_h}\sdot\frac{\partial \alpha_h}{\partial \nu_{ih}}=b_h(1-b_h)x_i
$$
​		因此，最终形式为：
$$
\frac{\partial E_k}{\partial\gamma_{ih}}= b_h(1-b_h)x_i\sum_{j=1}^{l}(g_j\omega_{hj})
$$
​		令，
$$
e_h = -\frac{\partial E_k}{\partial b_h}\sdot\frac{\partial b_h}{\partial \alpha_h}\\
=b_h(1-b_h)\sum_{j=1}^{l}(g_j\omega_{hj})
$$
​		由是，
$$
\Delta\nu_{ih}=\eta e_hx_i
$$



* ## 输出层的偏置项 $\theta_j$ 

  ​		与上面推导类似的， $\hat{y}_j^k = f(\beta_j-\theta_j)$
  $$
  \Delta\theta_{j}=-\eta\frac{\partial E_k}{\partial \theta_j}\\
  \frac{\partial E_k}{\partial \theta_j}=\frac{\partial E_k}{\partial \hat{y}_j^k}\sdot\frac{\partial \hat{y}_j^k}{\partial \theta_j}
  =(y_j^k-\hat{y}_j^k)\hat{y}_j^k(1-\hat{y}_j^k)
  $$
  ​		因此
  $$
  \Delta\theta_{j}=-\eta\frac{\partial E_k}{\partial \theta_j}=-\eta g_j
  $$
  



* ## 隐层的偏置项 $\gamma_h$ 

$$
\frac{\partial E_k}{\partial\gamma_{ih}}=\sum_{j=1}^{l}\frac{\partial E_k}{\partial \hat{y}_j^k}\sdot\frac{\partial \hat{y}_j^k}{\partial \beta_j}\sdot\frac{\partial \beta_j}{\partial b_h}\sdot\frac{\partial b_h}{\partial \gamma_h}
$$

​		由（13）（14）可知，
$$
\sum_{j=1}^{l}\frac{\partial E_k}{\partial \hat{y}_j^k}\sdot\frac{\partial \hat{y}_j^k}{\partial \beta_j}\sdot\frac{\partial \beta_j}{\partial b_h}=\sum_{j=1}^{l}(g_j\omega_{hj})
$$
​		其中，
$$
g_j =-\frac{\partial E_k}{\partial \hat{y}_j^k}\sdot\frac{\partial \hat{y}_j^k}{\partial \beta_j}\\
=-(\hat{y}_j^k-y_j^k)f'(\beta_j-\theta_j)
$$
​		由于 $b_h=f(\alpha_h-\gamma_h)$  和（7）
$$
\frac{\partial b_h}{\partial \gamma_h}=b_h(1-b_h)(-1)
$$
​		因此，
$$
\frac{\partial E_k}{\partial\gamma_{ih}}=-b_h(1-b_h)\sum_{j=1}^{l}(g_j\omega_{hj})
$$
​		利用（17）式简化后得到：
$$
\Delta\gamma_h=-\eta e_h
$$


​		至此，对于包含一个隐层的神经网络中所有参数的关于均方误差的导数结果都求出来了，可根据这些导数进行梯度下降，更新各个数值。
$$
\Delta\omega_{hj}=\eta g_jb_h\\
\Delta\theta_{j}=-\eta g_j\\
\Delta\nu_{ih}=\eta e_hx_i\\
\Delta\gamma_h=-\eta e_h
$$

---

输入： 训练集 $D=\{(x_k,y_k)\}^m_{k=1}$ ，学习率 $\eta$ 

过程：

1：在 $(0,1)$ 范围内随机初始化网络中所有连接权值和阈值

2：repeat

3：	for all $(x_k,y_k)\in D$ do 

4：		根据当前参数和式（1）计算 $\hat{y}_k$ 

5：		根据式（9）计算输出层神经元梯度项 $g_i$ 

6：		根据式（17）计算隐层神经元梯度项 $e_h$

7：		根据式（3）（28）更新连接权值 $\omega_{hj},\nu_{ih}$ 和阈值 $\theta_j,\gamma_h$ 

8：	end for

9：until 达到停止条件

输出连接权值与阈值确定的多层前馈神经网络

---

​		注意，本节推导的反向传播算法的 cost function 基于（1）式的均方误差，实例应用中一般会选择更加复杂的 cost function，例如在 Logistitc 回归中的带有 log 函数的代价函数，具体的推导将在下一节给出。